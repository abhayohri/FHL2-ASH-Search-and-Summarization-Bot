{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from configparser import ConfigParser\n",
    "from flask import Flask, request,  jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "import openai\n",
    "import json\n",
    "\n",
    "parser = ConfigParser()\n",
    "parser.read('../secrets.cfg')\n",
    "\n",
    "AZURE_SEARCH_API_VERSION = parser.get('my_api','AZURE_SEARCH_API_VERSION')\n",
    "AZURE_OPENAI_API_VERSION = parser.get('my_api','AZURE_OPENAI_API_VERSION')\n",
    "AZURE_SEARCH_ENDPOINT = parser.get('my_api','AZURE_SEARCH_ENDPOINT')\n",
    "AZURE_SEARCH_KEY = parser.get('my_api','AZURE_SEARCH_KEY')\n",
    "SEMANTIC_CONFIG = parser.get('my_api','SEMANTIC_CONFIG')\n",
    "AZURE_OPENAI_ENDPOINT = parser.get('my_api','AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_API_KEY = parser.get('my_api','AZURE_OPENAI_API_KEY')\n",
    "PORTAL_TOKEN = parser.get('my_api','PORTAL_TOKEN')\n",
    "URL  = parser.get('my_api','ASH_EXAMPLE_DATA_SOURCE')\n",
    "MODEL = \"gpt-35-turbo\" # options: gpt-35-turbo, gpt-4, gpt-4-32k\n",
    "\n",
    "SEARCH_STRATEGY = \"azure-search\" #('azure-search', 'ash-api', 'azure-search-similarity-embedder')\n",
    "\n",
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"] = AZURE_OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\" \n",
    "\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': AZURE_SEARCH_KEY}\n",
    "ASHheaders = {\"Authorization\": PORTAL_TOKEN}\n",
    "indexes = [\"servicehealthfhl-index1\"]\n",
    "\n",
    "\n",
    "# mode = \"Jupyter\"\n",
    "mode = \"Service\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from common.model_common import model_tokens_limit, num_tokens_from_string, num_tokens_from_docs\n",
    "from common.azure_search_helper import get_formatted_azure_search_results, sort_and_order_content\n",
    "from common.ash_events_helper import fetch_events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api-endpoint\n",
    "URL  = parser.get('my_api','ASH_EXAMPLE_DATA_SOURCE') \n",
    "\n",
    "if mode == \"Jupyter\":\n",
    "    ash_data = fetch_events_data(URL, ASHheaders)\n",
    "    print (ash_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create langchain documents and get the most recent documents that are within the token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langchain_documents(ordered_content):# Iterate over each of the results chunks and create a LangChain Document class to use further in the pipeline\n",
    "    docs = []\n",
    "    for key,value in ordered_content.items():\n",
    "        docs.append(Document(page_content=value[\"title\"], metadata={\"source\": key}))\n",
    "        for page in value[\"chunks\"]:\n",
    "            docs.append(Document(page_content=page, metadata={\"source\": key}))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate number of tokens of our docs\n",
    "def get_token_sizes(docs):\n",
    "    tokens_limit = model_tokens_limit(MODEL) \n",
    "    if(len(docs)>0):        \n",
    "        num_tokens = num_tokens_from_docs(docs) \n",
    "        print(\"Custom token limit for\", MODEL, \":\", tokens_limit)\n",
    "        print(\"Combined docs tokens count:\",num_tokens)\n",
    "        return tokens_limit, num_tokens    \n",
    "    else:\n",
    "        print(\"NO RESULTS FROM AZURE SEARCH\")\n",
    "        return tokens_limit,0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"Jupyter\":\n",
    "    # Create our LLM model\n",
    "    # Make sure you have the deployment named \"gpt-35-turbo\" for the model \"gpt-35-turbo (0301)\". \n",
    "    # Use \"gpt-4\" if you have it available.\n",
    "    llm2 = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulate the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"Jupyter\":\n",
    "    QUESTION = \"what are the Authentication issues?\" # the question asked by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMT_TEMPLATE = \"\"\"Below is a history of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base about Azure outages and service issues.\n",
    "    Generate a search query based on the conversation and the new question. \n",
    "    Do not include cited source filenames and document names e.g info.txt or doc.pdf in the search query terms.\n",
    "    Do not include any text inside [] or <<>> in the search query terms.\n",
    "    If the question is not in English, translate the question to English before generating the search query.\n",
    "=========\n",
    "Chat History:\n",
    "{}\n",
    "=========\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Search query:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_query_from_history(question, chat_history, debug= False):\n",
    "    if debug:\n",
    "        print (\"method: generate_query_from_history\")\n",
    "        print (\"chat_history: \", chat_history)\n",
    "    \n",
    "    answer = \"\"\n",
    "    if chat_history == \"\":\n",
    "        print (\"Not calling GPT to make query\")\n",
    "        return answer\n",
    "\n",
    "    completion = openai.Completion.create(\n",
    "            engine=MODEL, \n",
    "            prompt=QUERY_PROMT_TEMPLATE.format(chat_history, question), \n",
    "            temperature=0.5, \n",
    "            max_tokens=32, \n",
    "            n=1, \n",
    "            stop=[\"\\n\"])\n",
    "    q = completion.choices[0].text\n",
    "\n",
    "    answer = q\n",
    "\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "# generate_query_from_history(QUESTION, chat_history = \"<|im_start|>user\\nwhen did I have authentication issues\\n<|im_end|>\\n<|im_start|>assistant\\nI don't know when you had authentication issues. None of the provided Tracking IDs mention a timeframe that matches the question. \\n<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_query(question, hisory_text = \"\"):\n",
    "    user_input = question\n",
    "\n",
    "    ## Define user input\n",
    "    search_query = user_input\n",
    "\n",
    "    ## TBD: add chat history/summary as query input\n",
    "    search_query = generate_query_from_history(user_input, hisory_text)\n",
    "    \n",
    "\n",
    "    if search_query == \"\":\n",
    "        search_query = user_input\n",
    "\n",
    "    print (\"search query: {}\".format(search_query))\n",
    "    return search_query\n",
    "\n",
    "\n",
    "if mode == \"Jupyter\":\n",
    "    search_query = get_search_query(QUESTION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: Azure Search    \n",
    "  \n",
    "The first Question the user asks:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To find what events might be associated with this Question. We need to search all the users events we do this currently via keyword cognitive search and by a limited in memory vector search.\n",
    "    Current logic:\n",
    "    get events via the index currently set to 5\n",
    "    order and sort by score\n",
    "    if too many events returned for tokenization vector sort to get top results currently 4. TO DISABLE THIS: get number of docs that can fit in a token size(untested)           \n",
    "    if 0 results retuned: search on a small subset of the most recent data(from the api) using the embeddings model. (Allow user to select next not implemented) \n",
    "    **This should all be replaced by doing a vector search using the in private preview version of cognitive search which has the capabilities of vector search baked in**\n",
    "\n",
    "1. The keyword search: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. call Azure search service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calls Azure Search service to get relevant documents based on the query.\n",
    "def get_azure_search_results(search_query, filter = None, skip = 0): # get the events the question might pertain to. currently gets 5 events in the example\n",
    "    \n",
    "    print (\"Calling Azure search to get relevant documents...\")\n",
    "    agg_search_results = [] \n",
    "\n",
    "    _skip = 5*skip\n",
    "    print(f\"skipping {_skip} documents\")\n",
    "\n",
    "    \n",
    "    for index in indexes:\n",
    "        url = AZURE_SEARCH_ENDPOINT + '/indexes/'+ index + '/docs'\n",
    "        url += '?api-version={}'.format(AZURE_SEARCH_API_VERSION)\n",
    "        url += '&search={}'.format(search_query.strip(\"\\\"\\'\"))\n",
    "        url += '&select=*'\n",
    "        url += '&$top=5'  # You can change this to anything you need/want\n",
    "        if filter != None:\n",
    "            url += '&filter={}'.format(filter)\n",
    "        url += '&queryLanguage=en-us'\n",
    "        url += '&queryType=semantic'\n",
    "        url += '&semanticConfiguration={}'.format(SEMANTIC_CONFIG)\n",
    "        url += '&$count=true'\n",
    "        url += '&speller=lexicon'\n",
    "        url += '&answers=extractive|count-3'\n",
    "        url += '&captions=extractive|highlight-false'\n",
    "        url += f'&$skip={_skip}'\n",
    "\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        print(url)\n",
    "        #print(resp.status_code)\n",
    "\n",
    "        search_results = resp.json()\n",
    "        print (\"search results:\", search_results)\n",
    "        agg_search_results.append(search_results)\n",
    "        \n",
    "        results_found = search_results['@odata.count']\n",
    "        returned_results = len(search_results['value'])\n",
    "        print(\"Results Found: {}, Results Returned: {}\".format(results_found, returned_results ))\n",
    "        \n",
    "        return agg_search_results, results_found, returned_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_azure_search_data_as_langchain_docs(search_query):\n",
    "    agg_search_results, results_found, returned_results = get_azure_search_results(search_query)\n",
    "    formatted_search_results = get_formatted_azure_search_results(agg_search_results)\n",
    "    ordered_search_results = sort_and_order_content(formatted_search_results)  # filter and order document by search score\n",
    "    #print(json.dumps(ordered_content, indent=4))\n",
    "    docs  = create_langchain_documents(ordered_search_results)\n",
    "    return docs\n",
    "\n",
    "\n",
    "if mode == \"Jupyter\":\n",
    "    docs = fetch_azure_search_data_as_langchain_docs(search_query)\n",
    "    print (\"Azure search results:\", docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: get data from ASH events API and do similarity search/vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def fetch_ASH_data_as_langchain_docs():\n",
    "    summary_data = fetch_events_data(URL, ASHheaders)\n",
    "    summary_docs  = create_langchain_documents(summary_data)\n",
    "    #summary_docs_used = limit_docs_to_max_token_lenght(summary_docs)\n",
    "    return summary_docs\n",
    "\n",
    "\n",
    "if mode == \"Jupyter\":\n",
    "    docs = fetch_ASH_data_as_langchain_docs()\n",
    "    print (docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def select_emedding_model(docs):\n",
    "    if len(docs) < 50:\n",
    "        # OpenAI models are accurate but slower, they also only (for now) accept one text at a time (chunk_size)\n",
    "        embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "    else:\n",
    "        # Bert based models are faster (3x-10x) but not as great in accuracy as OpenAI models\n",
    "        # Since this repo supports Multiple languages we need to use a multilingual model. \n",
    "        # But if English only is the requirement, use \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "        # The fastest english model is \"all-MiniLM-L12-v2\"\n",
    "        embedder = HuggingFaceEmbeddings(model_name = 'distiluse-base-multilingual-cased-v2') #not deployed\n",
    "    \n",
    "    print(embedder)\n",
    "\n",
    "def find_most_relevant_docs(search_query, docs):\n",
    "    if (len(docs) == 0):\n",
    "        return docs\n",
    "    embedder = select_emedding_model(docs)\n",
    "\n",
    "    # Create our in-memory vector database index from the chunks given by Azure Search.\n",
    "    # We are using FAISS. https://ai.facebook.com/tools/faiss/\n",
    "    db = FAISS.from_documents(docs, embedder)\n",
    "    top_docs = db.similarity_search(search_query, k=4)  # Return the top 4 documents\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_search(search_query, search_strategy):\n",
    "    if search_strategy == 'azure-search':\n",
    "        docs = fetch_azure_search_data_as_langchain_docs(search_query)\n",
    "        print(f\"number of docs returned by azure search: {len(docs)}\" )\n",
    "        top_docs = docs\n",
    "    elif search_strategy == \"ash-api\":\n",
    "        docs = fetch_ASH_data_as_langchain_docs()\n",
    "        print(f\"number of docs returned by api: {len(docs)}\" )\n",
    "        top_docs = find_most_relevant_docs(search_query, docs)\n",
    "    elif search_strategy == 'azure-search-similarity-embedder':\n",
    "        docs = fetch_azure_search_data_as_langchain_docs(search_query)\n",
    "        print(f\"number of docs returned by azure search: {len(docs)}\" )\n",
    "        top_docs = find_most_relevant_docs(search_query, docs)\n",
    "        print(f\"the top docs selected by similarity search: ${len(top_docs)}\" )\n",
    "\n",
    "    tokens_limit,num_tokens = get_token_sizes(docs)\n",
    "    chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"\n",
    "    return top_docs, chain_type, True\n",
    "\n",
    "\n",
    "if mode == \"Jupyter\":\n",
    "    search_strategy = SEARCH_STRATEGY\n",
    "    print(\"search_strategy: \", SEARCH_STRATEGY)\n",
    "    top_docs,chain_type,search_complete = do_search(search_query, SEARCH_STRATEGY)\n",
    "    print(\"Chain Type selected:\", chain_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The vector search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain_type_and_top_docs(question, tokens_limit,num_tokens,docs):    \n",
    "    print(num_tokens)\n",
    "    search_complete =False\n",
    "    if num_tokens ==0 or docs is None or len(docs) == 0 or num_tokens > tokens_limit: # need to do a vector search in these cases\n",
    "        if num_tokens ==0 or docs is None or len(docs) == 0:\n",
    "            search_complete =True\n",
    "            docs = fetch_ASH_data_as_langchain_docs()\n",
    "            print(f\"number of docs returned by api: {len(docs)}\" )\n",
    "            if docs is None or len(docs) == 0:\n",
    "                return None,\"\",True\n",
    "        # Select the Embedder model\n",
    "        if len(docs) < 50:\n",
    "            # OpenAI models are accurate but slower, they also only (for now) accept one text at a time (chunk_size)\n",
    "            embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "        else:\n",
    "            # Bert based models are faster (3x-10x) but not as great in accuracy as OpenAI models\n",
    "            # Since this repo supports Multiple languages we need to use a multilingual model. \n",
    "            # But if English only is the requirement, use \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "            # The fastest english model is \"all-MiniLM-L12-v2\"\n",
    "            embedder = HuggingFaceEmbeddings(model_name = 'distiluse-base-multilingual-cased-v2') #not deployed\n",
    "        \n",
    "        print(embedder)\n",
    "        \n",
    "        # Create our in-memory vector database index from the chunks given by Azure Search.\n",
    "        # We are using FAISS. https://ai.facebook.com/tools/faiss/\n",
    "        db = FAISS.from_documents(docs, embedder)\n",
    "        top_docs = db.similarity_search(question, k=4)  # Return the top 4 documents\n",
    "        print(f\"the top docs selected by similarity search: ${len(top_docs)}\" )\n",
    "        \n",
    "        # Now we need to recalculate the tokens count of the top results from similarity vector search\n",
    "        # in order to select the chain type: stuff (all chunks in one prompt) or \n",
    "        # map_reduce (multiple calls to the LLM to summarize/reduce the chunks and then combine them)\n",
    "        \n",
    "        num_tokens = num_tokens_from_docs(top_docs)\n",
    "        print(\"Token count after similarity search:\", num_tokens)\n",
    "        chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"\n",
    "        \n",
    "    else:\n",
    "        # if total tokens is less than our limit, we don't need to vectorize and do similarity search\n",
    "        top_docs = docs\n",
    "        chain_type = \"stuff\"\n",
    "    \n",
    "    \n",
    "    return top_docs,chain_type,search_complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search is complete time to Summarize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_template():\n",
    "    template_start = \"\"\"\n",
    "    These are examples of how you must provide the answer:\n",
    "    --> Beginning of examples\n",
    "    \"\"\"\n",
    "    template_end = \"\"\"\n",
    "    <-- End of examples\n",
    "    DO NOT cite the examples above in your response. Instead, reference the information below.\n",
    "    \n",
    "    Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "    If you don't know the answer, just say \"Sorry, I couldn't find a clear answer for this.\". Don't try to make up an answer.\n",
    "    ALWAYS return a \"SOURCES\" part in your answer.\n",
    "    Instead of using the word \"Document\" use \"event\"\n",
    "    Instead of using the word \"Source\" use \"Tracking ID\"\n",
    "    Respond in {language}.\n",
    "    =========\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    =========\n",
    "    QUESTION: {question}\n",
    "    =========\n",
    "    {summaries}\n",
    "    =========\n",
    "    FINAL ANSWER IN {language}:\"\"\"\n",
    "\n",
    "    examples_string = \"\"\"\"\"\"\n",
    "    f = open(\"prompt_examples.json\" , \"rb\" )\n",
    "    examples = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    for example in examples:\n",
    "        examples_string += \"\\n=========\"\n",
    "        examples_string += \"\\nQUESTION: \" + example['question']\n",
    "        examples_string += \"\\n=========\"\n",
    "        for document in example['documents']:\n",
    "            examples_string += \"\\nContent: \" + document['content']\n",
    "            examples_string += \"\\nSource: \" + document['source']\n",
    "        examples_string += \"\\n=========\"\n",
    "        examples_string += \"\\nFINAL ANSWER IN English: \" + example['answer']\n",
    "        examples_string += \"\\nSOURCES: \" + example['answer_sources']\n",
    "\n",
    "    print(examples_string)\n",
    "\n",
    "    return template_start + examples_string + template_end\n",
    "\n",
    "COMBINE_PROMPT_TEMPLATE = create_prompt_template()\n",
    "\n",
    "COMBINE_PROMPT = PromptTemplate(\n",
    "    template=COMBINE_PROMPT_TEMPLATE, input_variables=[\"summaries\", \"question\", \"language\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remember history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_arr = [] #history array\n",
    "answer = \"\"\n",
    "\n",
    "def add_history(history_arr, user_input= \"\", answer = \"\"):\n",
    "    \n",
    "    if user_input == None or user_input == \"\":\n",
    "        return history_arr\n",
    "    \n",
    "    history_dict = {}\n",
    "    history_dict['user'] = user_input\n",
    "    history_dict['bot'] = answer\n",
    "    history_arr.append(history_dict)\n",
    "    return history_arr\n",
    "\n",
    "\n",
    "\n",
    "def get_chat_history_as_text(history, include_last_turn=True, approx_max_tokens=1000) -> str:\n",
    "        history_text = \"\"\n",
    "        for h in reversed(history if include_last_turn else history[:-1]):\n",
    "            history_text = \"\"\"<|im_start|>user\"\"\" +\"\\n\" + h[\"user\"] + \"\\n\" + \"\"\"<|im_end|>\"\"\" + \"\\n\" + \"\"\"<|im_start|>assistant\"\"\" + \"\\n\" + (h.get(\"bot\") + \"\"\"<|im_end|>\"\"\" if h.get(\"bot\") else \"\") + \"\\n\" + history_text\n",
    "            if len(history_text) > approx_max_tokens*4:\n",
    "                break\n",
    "        print (\"chat_history: \", history_text)\n",
    "        return history_text\n",
    "\n",
    "\n",
    "if mode == \"Jupyter\":\n",
    "    h_arr = add_history(h_arr, QUESTION, answer)\n",
    "    print (h_arr)\n",
    "    history_text = get_chat_history_as_text(h_arr)\n",
    "    print (\"chat_history: \", history_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_response(question,llm2,chain_type,top_docs, chat_history):    \n",
    "    if top_docs is not None and len(top_docs)> 0:\n",
    "        if chain_type == \"stuff\":\n",
    "            chain = load_qa_with_sources_chain(llm2, chain_type=chain_type, \n",
    "                                            prompt=COMBINE_PROMPT)\n",
    "        elif chain_type == \"map_reduce\":\n",
    "            chain = load_qa_with_sources_chain(llm2, chain_type=chain_type, \n",
    "                                            question_prompt=COMBINE_QUESTION_PROMPT,\n",
    "                                            combine_prompt=COMBINE_PROMPT,\n",
    "                                            return_intermediate_steps=True)\n",
    "\n",
    "        response = chain({\"input_documents\": top_docs, \"question\": question, \"language\": \"English\", \"chat_history\": chat_history})\n",
    "        answer = response['output_text']\n",
    "        print(response)\n",
    "        print(\"GPT output:\", answer)\n",
    "        return answer\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if mode == \"Jupyter\":\n",
    "    # Create our LLM model\n",
    "    # Make sure you have the deployment named \"gpt-35-turbo\" for the model \"gpt-35-turbo (0301)\". \n",
    "    # Use \"gpt-4\" if you have it available.\n",
    "    llm2 = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=500)\n",
    "    answer = get_chat_response(search_query,llm2,chain_type,top_docs, history_text)\n",
    "\n",
    "    display(HTML('<h4>Azure OpenAI ChatGPT Answer:</h4>'))\n",
    "    display(HTML(answer.split(\"SOURCES:\")[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get the sources:\n",
    "if mode == \"Jupyter\":\n",
    "    \n",
    "    sources_list = answer.split(\"SOURCES:\")[1].replace(\" \",\"\").split(\",\")\n",
    "    sources_html = '<u>Sources</u>: '\n",
    "    display(HTML(sources_html))\n",
    "    for index, value in enumerate(sources_list):\n",
    "        print(value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the flask service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Orchestrate search app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wrapper(question,skip, history_text :str, search_strategy: str):\n",
    "\n",
    "    search_query = get_search_query(question, history_text)\n",
    "    \n",
    "    top_docs,chain_type,search_complete = do_search(search_query, search_strategy)\n",
    "    \n",
    "\n",
    "    print ('search complete in do_search...')\n",
    "    return top_docs,chain_type,search_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validates and grounds the answer\n",
    "def get_answer_and_sources(gpt_output):\n",
    "    print (\"gpt_output\", gpt_output)\n",
    "    answer = \"\"\n",
    "    sources = \"\"\n",
    "    \n",
    "    try:\n",
    "        answer = gpt_output.split(\"SOURCES:\")[0]\n",
    "    except:\n",
    "        answer = \"\"\n",
    "\n",
    "    try:\n",
    "        sources = gpt_output.split(\"SOURCES:\")[1].replace(\" \",\"\").split(\",\")\n",
    "    except: \n",
    "        sources = \"\"\n",
    "\n",
    "    if answer == \"\":\n",
    "        answer = \"I'm sorry, I couldn't find relevant information. Please try asking again.\"    \n",
    "    return answer, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## orchestration layer\n",
    "def orchestrate_simple_chat(llm3, question: str, skip: int, history_arr: list, search_strategy: str):\n",
    "\n",
    "    # convert history into text.\n",
    "    chat_history = get_chat_history_as_text(history_arr)\n",
    "    \n",
    "    #search\n",
    "    top_docs3,chain_type3,search_complete = search_wrapper(question, skip, chat_history, search_strategy)\n",
    "\n",
    "    #prompt chatgpt               \n",
    "    gpt_output = get_chat_response(question,llm3,chain_type3,top_docs3, chat_history)\n",
    "    answer, sources = get_answer_and_sources(gpt_output)\n",
    "\n",
    "    history_arr =  add_history(history_arr,question, answer)      \n",
    "    print (\"chat_history: \", history_arr)\n",
    "    \n",
    "\n",
    "    response = jsonify(answer = answer , source_tracking_ids = sources, next_skip = skip +1, search_complete = search_complete)\n",
    "\n",
    "    return response, history_arr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Response\n",
    "# flask service\n",
    "\n",
    "if mode == \"Service\":\n",
    "\n",
    "    \n",
    "    app = Flask(__name__)\n",
    "    cors = CORS(app)\n",
    "    app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "\n",
    "    HTTP_400_BAD_REQUEST = 400\n",
    "\n",
    "    llm3 = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=500)\n",
    "    history_arr = []\n",
    "\n",
    "    @app.route('/askQuestion/')\n",
    "    @cross_origin()\n",
    "    def ask_question():\n",
    "        print (\"REQUEST: \", request)\n",
    "\n",
    "        if request.args is None:\n",
    "            return jsonify({'error': \"No question asked\"}),HTTP_400_BAD_REQUEST    \n",
    "\n",
    "        question = request.args.get('question')\n",
    "        if question is None or len(question) == 0: \n",
    "            return jsonify({'error': \"No question asked\"}),HTTP_400_BAD_REQUEST\n",
    "        \n",
    "        \n",
    "        skip = 0\n",
    "        try:\n",
    "            skip = int(request.args.get('skip'))\n",
    "        except:\n",
    "            skip = 0\n",
    "\n",
    "        search_complete = False\n",
    "        global history_arr\n",
    "        print(f\"the question is: {question}\")\n",
    "        print(f\"skip is: {skip}\")\n",
    "             \n",
    "        json_response, history_arr = orchestrate_simple_chat(llm3, question, skip, history_arr, search_strategy= SEARCH_STRATEGY)\n",
    "        print (\"=====================\")\n",
    "        \n",
    "        \n",
    "        return json_response\n",
    "\n",
    "    @app.route('/hello/', methods=['GET'])\n",
    "    @cross_origin()\n",
    "    def hello_world():\n",
    "        print (\"REQUEST: \", request)\n",
    "        response = Response()\n",
    "        response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "        return response\n",
    "    \n",
    "    app.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample:\n",
    "\n",
    "http://127.0.0.1:5000/askQuestion/?question=%22what%20are%20Authentication%20issues%22\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "    \"answer\":\"Authentication issues refer to difficulties experienced by customers when attempting to access Azure, Dynamics 365, and/or Microsoft 365 due to platform issues or third-party push notification service errors. The causes of these issues are identified in the Tracking IDs RV5D-7S0, F_BK-398, and FKPH-7Z8. The responses to these issues include mitigating the impact, increasing instance counts, routing traffic to other regions, and failing over to the legacy channel. Microsoft is taking steps to improve resiliency, monitoring, and documentation to prevent or reduce the impact of future incidents. Customers can also evaluate the reliability of their applications and configure Azure Service Health alerts to be notified of future service issues. \\n\",\n",
    "    \"num_tracking_ids\":4,\n",
    "    \"search_complete\":true,\n",
    "    \"source_tracking_ids\":[\"RV5D-7S0\",\"F_BK-398\",\"FKPH-7Z8\"]}\n",
    "\n",
    "{\"answer\":\"Authentication issues refer to difficulties experienced by customers when attempting to access Azure, Dynamics 365, and/or Microsoft 365 due to platform issues or third-party push notification service errors. The causes of these issues are identified in the Tracking IDs RV5D-7S0, F_BK-398, and FKPH-7Z8. The sources provide detailed information on what went wrong, how Microsoft responded, and the steps being taken to improve the service and make incidents like this less likely or less impactful. \\n\",\"num_tracking_ids\":4,\"search_complete\":false,\"source_tracking_ids\":[\"RV5D-7S0\",\"F_BK-398\",\"FKPH-7Z8\"]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
